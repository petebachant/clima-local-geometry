==========================================
LocalGeometry CUDA Benchmark - SLURM Job
==========================================
Job ID: 189813
Node: clima
Start time: Tue Feb 24 09:18:55 PST 2026

Loading climacommon/2025_05_15
  Loading requirement: climaauth/2025_05_15 openmpi/4.1.5-mpitrampoline
    julia/1.11.5 cuda/julia-pref
┌ Warning: CUDA runtime library `libcublasLt.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libcublasLt.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libnvJitLink.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/usr/local/cuda/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
Status `~/calkit/clima-local-geometry/.calkit/envs/main/Project.toml`
  [79e6a3ab] Adapt v4.4.0
  [4c88cf16] Aqua v0.8.14
  [c7e460c6] ArgParse v1.2.0
  [2119f1ac] AssociatedLegendrePolynomials v1.0.2
  [aae01518] BandedMatrices v1.11.0
  [6e4b80f9] BenchmarkTools v1.6.3
  [8e7c35d0] BlockArrays v1.9.3
⌃ [052768ef] CUDA v5.8.5
  [3a4d1b5c] ClimaComms v0.6.10
  [d414da3d] ClimaCore v0.14.50
  [cf7c7e5a] ClimaCorePlots v0.2.11
  [d934ef94] ClimaCoreTempestRemap v0.3.18
  [c8b6d40d] ClimaCoreVTK v0.7.6
  [5c42b081] ClimaParams v1.0.13
⌃ [595c0a79] ClimaTimeSteppers v0.8.5
  [5ae59095] Colors v0.13.1
  [1db9610d] CountFlops v0.1.0
  [7445602f] CubedSphere v0.3.4
⌅ [864edb3b] DataStructures v0.18.22
  [459566f4] DiffEqCallbacks v4.12.0
  [ffbed154] DocStringExtensions v0.9.5
  [7034ab61] FastBroadcast v0.3.5
  [f6369f11] ForwardDiff v1.3.2
  [d54b0c1a] GaussQuadrature v0.5.8
  [88fa7841] GilbertCurves v0.1.0
  [8197267c] IntervalSets v0.7.13
⌅ [c3a54625] JET v0.9.20
  [033835bb] JLD2 v0.6.3
⌅ [0b1a1467] KrylovKit v0.8.3
  [9dccce8e] LazyBroadcast v1.0.0
  [da04e1cc] MPI v0.20.23
⌃ [85f8d34a] NCDatasets v0.14.8
  [5da4648a] NVTX v1.0.3
  [0d71be07] NullBroadcasts v0.1.0
  [669c94d9] OrdinaryDiffEqSSPRK v1.11.0
  [b1df2697] OrdinaryDiffEqTsit5 v1.9.0
  [91a5bcdd] Plots v1.41.6
⌅ [08abe8d2] PrettyTables v2.4.0
  [efd6af41] ProfileCanvas v0.1.7
  [33c8b6b6] ProgressLogging v0.1.6
  [1fd47b50] QuadGK v2.11.2
  [731186ca] RecursiveArrayTools v3.48.0
  [1bc83da4] SafeTestsets v0.1.0
  [0bca4576] SciMLBase v2.144.0
⌅ [aa65fe97] SnoopCompile v3.1.4
⌅ [e2b509da] SnoopCompileCore v3.0.0
  [90137ffa] StaticArrays v1.9.17
  [10745b16] Statistics v1.11.1
  [2913bbd2] StatsBase v0.34.10
  [5d786b92] TerminalLoggers v0.1.7
  [b60c26fb] Thermodynamics v0.15.8
  [ac1d9e8a] ThreadsX v0.1.12
  [37e2e46d] LinearAlgebra v1.11.0
  [56ddb016] Logging v1.11.0
  [9abbd945] Profile v1.11.0
  [9a3f8284] Random v1.11.0
  [2f01184e] SparseArrays v1.11.0
  [8dfed614] Test v1.11.0
Info Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`
Julia version:
julia version 1.11.5

CUDA devices available:
name, memory.total [MiB]
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB

Julia project: .calkit/envs/main

Instantiating Julia environment...
==========================================

Running benchmark...
==========================================
┌ Warning: CUDA runtime library `libcublasLt.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libcublasLt.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libnvJitLink.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/usr/local/cuda/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218

======================================================================
LOCALGEOMETRY CUDA KERNEL PERFORMANCE BENCHMARK
======================================================================

======================================================================
INLINING VERIFICATION
======================================================================

Sizeof checks (should help predict inlining threshold):
  LocalGeometry field element:  8 bytes
  TwoFieldGeom:                 16 bytes
  FourFieldGeom:                32 bytes
  EightFieldGeom:               64 bytes
  SixteenFieldGeom:             128 bytes

None field overhead testing:
  TwoFieldWithNothingGeom:      16 bytes
  FourFieldPartiallyNothingGeom: 16 bytes
  FullGeomWithOptionals:        48 bytes (FT sentinel)
  MinimalGeomWithPadding:       16 bytes

Note: CUDA typically inlines structs < 128 bytes effectively
Note: sizeof(Nothing) = 0 bytes

======================================================================
SECTION 1: Basic Geometry Access Patterns
======================================================================

SECTION 2: Testing impact of struct size on inlining

SECTION 2B: Testing overhead of nothing fields vs simplified structs

SECTION 3: Testing projection operations (common in physics kernels)

Running benchmarks (this takes ~2-3 minutes)...

(1/24) benchmarking "9d_full_geom_with_optionals"...
done (took 1.158046241 seconds)
(2/24) benchmarking "2f_f_x_lg"...
done (took 1.370163454 seconds)
(3/24) benchmarking "2h_f_x_lg_noinline"...
done (took 1.106686587 seconds)
(4/24) benchmarking "6_two_field_access"...
done (took 0.670355775 seconds)
(5/24) benchmarking "1_baseline_simple"...
done (took 0.616760372 seconds)
(6/24) benchmarking "2b_pointwise_lg_j"...
done (took 1.085122547 seconds)
(7/24) benchmarking "10_vector_baseline"...
done (took 1.256133519 seconds)
(8/24) benchmarking "8_eight_field_access"...
done (took 0.638468552 seconds)
(9/24) benchmarking "2_full_lg_jacobian"...
done (took 0.629157334 seconds)
(10/24) benchmarking "7_four_field_access"...
done (took 0.644951742 seconds)
(11/24) benchmarking "2c_pointwise_lg_j_stack"...
done (took 1.14305384 seconds)
(12/24) benchmarking "3_full_lg_multiple"...
done (took 1.139252378 seconds)
(13/24) benchmarking "5_simplified_lg"...
done (took 1.045339459 seconds)
(14/24) benchmarking "9e_minimal_with_padding"...
done (took 0.725176953 seconds)
(15/24) benchmarking "9b_two_field_with_nothing"...
done (took 0.648019378 seconds)
(16/24) benchmarking "2e_fd_localgeom_constructor"...
done (took 1.621449418 seconds)
(17/24) benchmarking "12_multiple_scalar_access"...
done (took 1.259717963 seconds)
(18/24) benchmarking "2d_pointwise_lg_j_noinline"...
done (took 1.17557316 seconds)
(19/24) benchmarking "2g_lambda_f_x_lg"...
done (took 1.173863578 seconds)
(20/24) benchmarking "11_project_full_lg"...
done (took 1.365103197 seconds)
(21/24) benchmarking "9_sixteen_field_access"...
done (took 0.640775711 seconds)
(22/24) benchmarking "9c_four_field_partial_nothing"...
done (took 0.636973609 seconds)
(23/24) benchmarking "4_extracted_j"...
done (took 0.663899085 seconds)
(24/24) benchmarking "2i_lambda_f_x_lg_noinline"...
done (took 1.074359516 seconds)

======================================================================
BENCHMARK RESULTS
======================================================================

SECTION 1: Basic Geometry Access
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  baseline_simple                     15.35 μs  (  +0.0% vs baseline)
  full_lg_jacobian                    17.04 μs  ( +11.0% vs baseline)
  2b_pointwise_lg_j                   16.64 μs  (  +8.4% vs baseline)
  2c_pointwise_lg_j_stack             17.06 μs  ( +11.1% vs baseline)
  2d_pointwise_lg_j_noinline          18.18 μs  ( +18.4% vs baseline)
  2e_fd_localgeom_constructor         13.64 μs  ( -11.1% vs baseline)
  2f_f_x_lg                           16.05 μs  (  +4.6% vs baseline)
  2g_lambda_f_x_lg                    15.94 μs  (  +3.8% vs baseline)
  2h_f_x_lg_noinline                  17.88 μs  ( +16.5% vs baseline)
  2i_lambda_f_x_lg_noinline           17.21 μs  ( +12.1% vs baseline)
  full_lg_multiple                    18.81 μs  ( +22.5% vs baseline)
  extracted_j                         16.68 μs  (  +8.7% vs baseline)
  simplified_lg                       15.84 μs  (  +3.2% vs baseline)

----------------------------------------------------------------------
SECTION 2: Struct Size Impact on Inlining
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  two_field_access                    16.92 μs  ( +10.2% vs baseline)
  four_field_access                   16.85 μs  (  +9.8% vs baseline)
  eight_field_access                  17.10 μs  ( +11.4% vs baseline)
  sixteen_field_access                16.94 μs  ( +10.4% vs baseline)

----------------------------------------------------------------------
SECTION 2B: Nothing Field Overhead Testing
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  9b_two_field_with_nothing           16.42 μs  (  +7.0% vs baseline)
  9c_four_field_partial_nothing       17.06 μs  ( +11.1% vs baseline)
  9d_full_geom_with_optionals         17.47 μs  ( +13.8% vs baseline)
  9e_minimal_with_padding             16.29 μs  (  +6.1% vs baseline)

----------------------------------------------------------------------
SECTION 3: Projection Operations
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  vector_baseline                     16.66 μs  (  +0.0% vs vec_baseline)
  project_full_lg                     16.77 μs  (  +0.7% vs vec_baseline)
  multiple_scalar_access              17.17 μs  (  +3.1% vs vec_baseline)

======================================================================
MEMORY FOOTPRINT COMPARISON
======================================================================

Data structure size per point:
  Scalar field:                    128.0 bytes
  TwoFieldGeom:                    256.0 bytes
  FourFieldGeom:                   512.0 bytes
  EightFieldGeom:                  1024.0 bytes
  SixteenFieldGeom:                2048.0 bytes
  Full LocalGeometry:              2688.0 bytes
  Extracted J:                     128.0 bytes

Total memory footprint:
  Scalar field:                    0.0001220703125 MB
  TwoFieldGeom:                    0.000244140625 MB (2.0x scalar)
  FourFieldGeom:                   0.00048828125 MB (4.0x scalar)
  EightFieldGeom:                  0.0009765625 MB (8.0x scalar)
  SixteenFieldGeom:                0.001953125 MB (16.0x scalar)
  Full LocalGeometry:              0.0025634765625 MB (21.0x scalar)
  Extracted J:                     0.0001220703125 MB (1.0x scalar)

======================================================================
ANALYSIS & KEY FINDINGS
======================================================================

1. BASIC GEOMETRY ACCESS OVERHEAD:
   Full LocalGeometry (J only):      11.0%
   Extracted J:                      8.7%

2. STRUCT SIZE IMPACT (accessing single field):
   TwoFieldGeom (16 bytes):          10.2%
   FourFieldGeom (32 bytes):         9.8%
   EightFieldGeom (64 bytes):        11.4%
   SixteenFieldGeom (128 bytes):     10.4%

   ✓ All struct sizes show similar performance - good inlining

3. PROJECTION OPERATIONS OVERHEAD:
   Covariant->Contravariant:         0.7%

======================================================================
RECOMMENDATIONS
======================================================================

⚠️  SIGNIFICANT OVERHEAD DETECTED:
   • Full LocalGeometry access has >10% overhead
   • Consider: Extract J/WJ at kernel entry

   ACTION: Refactor hot paths to use simplified geometry types

======================================================================
CODE INSPECTION NOTES
======================================================================
To verify compiler inlining behavior:

1. Check LLVM IR for a simple kernel:
   julia> f(x, lg) = x + lg.J
   julia> @code_llvm f(1.0, first(local_geom_full))

   Look for: Should see direct field access, not function calls

2. Check PTX assembly for CUDA kernels:
   julia> using CUDA
   julia> kernel(x, lg) = (@inbounds x[1] += lg[1].J; nothing)
   julia> @device_code_ptx kernel(CuArray([1.0]), parent(local_geom_full))

   Look for: ld.param instructions (should be minimal for inlined structs)

3. Use nsys to profile actual memory bandwidth:
   ./scripts/run-nsys.sh --output=results/nsys/benchmark_lg \
       julia --project scripts/benchmark_local_geometry_impact.jl

   Then analyze with:
   nsys stats results/nsys/benchmark_lg.nsys-rep

   Look for: Memory bandwidth utilization, kernel occupancy

======================================================================

Markdown results written to: /home/pbachant/calkit/clima-local-geometry/scripts/../results/benchmark_local_geometry_impact.md

==========================================
Benchmark complete
End time: Tue Feb 24 09:21:06 PST 2026
==========================================
