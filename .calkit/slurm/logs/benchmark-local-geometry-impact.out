==========================================
LocalGeometry CUDA Benchmark - SLURM Job
==========================================
Job ID: 189807
Node: clima
Start time: Tue Feb 24 06:33:34 PST 2026

Loading climacommon/2025_05_15
  Loading requirement: climaauth/2025_05_15 openmpi/4.1.5-mpitrampoline
    julia/1.11.5 cuda/julia-pref
┌ Warning: CUDA runtime library `libcublasLt.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libcublasLt.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libnvJitLink.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/usr/local/cuda/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
Status `~/calkit/clima-local-geometry/.calkit/envs/main/Project.toml`
  [79e6a3ab] Adapt v4.4.0
  [4c88cf16] Aqua v0.8.14
  [c7e460c6] ArgParse v1.2.0
  [2119f1ac] AssociatedLegendrePolynomials v1.0.2
  [aae01518] BandedMatrices v1.11.0
  [6e4b80f9] BenchmarkTools v1.6.3
  [8e7c35d0] BlockArrays v1.9.3
⌃ [052768ef] CUDA v5.8.5
  [3a4d1b5c] ClimaComms v0.6.10
  [d414da3d] ClimaCore v0.14.50
  [cf7c7e5a] ClimaCorePlots v0.2.11
  [d934ef94] ClimaCoreTempestRemap v0.3.18
  [c8b6d40d] ClimaCoreVTK v0.7.6
  [5c42b081] ClimaParams v1.0.13
⌃ [595c0a79] ClimaTimeSteppers v0.8.5
  [5ae59095] Colors v0.13.1
  [1db9610d] CountFlops v0.1.0
  [7445602f] CubedSphere v0.3.4
⌅ [864edb3b] DataStructures v0.18.22
  [459566f4] DiffEqCallbacks v4.12.0
  [ffbed154] DocStringExtensions v0.9.5
  [7034ab61] FastBroadcast v0.3.5
  [f6369f11] ForwardDiff v1.3.2
  [d54b0c1a] GaussQuadrature v0.5.8
  [88fa7841] GilbertCurves v0.1.0
  [8197267c] IntervalSets v0.7.13
⌅ [c3a54625] JET v0.9.20
  [033835bb] JLD2 v0.6.3
⌅ [0b1a1467] KrylovKit v0.8.3
  [9dccce8e] LazyBroadcast v1.0.0
  [da04e1cc] MPI v0.20.23
⌃ [85f8d34a] NCDatasets v0.14.8
  [5da4648a] NVTX v1.0.3
  [0d71be07] NullBroadcasts v0.1.0
  [669c94d9] OrdinaryDiffEqSSPRK v1.11.0
  [b1df2697] OrdinaryDiffEqTsit5 v1.9.0
  [91a5bcdd] Plots v1.41.6
⌅ [08abe8d2] PrettyTables v2.4.0
  [efd6af41] ProfileCanvas v0.1.7
  [33c8b6b6] ProgressLogging v0.1.6
  [1fd47b50] QuadGK v2.11.2
  [731186ca] RecursiveArrayTools v3.48.0
  [1bc83da4] SafeTestsets v0.1.0
  [0bca4576] SciMLBase v2.144.0
⌅ [aa65fe97] SnoopCompile v3.1.4
⌅ [e2b509da] SnoopCompileCore v3.0.0
  [90137ffa] StaticArrays v1.9.17
  [10745b16] Statistics v1.11.1
  [2913bbd2] StatsBase v0.34.10
  [5d786b92] TerminalLoggers v0.1.7
  [b60c26fb] Thermodynamics v0.15.8
  [ac1d9e8a] ThreadsX v0.1.12
  [37e2e46d] LinearAlgebra v1.11.0
  [56ddb016] Logging v1.11.0
  [9abbd945] Profile v1.11.0
  [9a3f8284] Random v1.11.0
  [2f01184e] SparseArrays v1.11.0
  [8dfed614] Test v1.11.0
Info Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`
Julia version:
julia version 1.11.5

CUDA devices available:
name, memory.total [MiB]
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB
NVIDIA A100-SXM4-80GB, 81920 MiB

Julia project: .calkit/envs/main

Instantiating Julia environment...
==========================================

Running benchmark...
==========================================
┌ Warning: CUDA runtime library `libcublasLt.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libcublasLt.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.13` was loaded from a system path, `/usr/local/cuda/lib64/libnvJitLink.so.13`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/usr/local/cuda/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA ~/.julia/packages/CUDA/TPbi4/src/initialization.jl:218

======================================================================
LOCALGEOMETRY CUDA KERNEL PERFORMANCE BENCHMARK
======================================================================

======================================================================
INLINING VERIFICATION
======================================================================

Sizeof checks (should help predict inlining threshold):
  LocalGeometry field element:  8 bytes
  TwoFieldGeom:                 16 bytes
  FourFieldGeom:                32 bytes
  EightFieldGeom:               64 bytes
  SixteenFieldGeom:             128 bytes

Note: CUDA typically inlines structs < 128 bytes effectively

======================================================================
SECTION 1: Basic Geometry Access Patterns
======================================================================

SECTION 2: Testing impact of struct size on inlining

SECTION 3: Testing projection operations (common in physics kernels)

Running benchmarks (this takes ~2-3 minutes)...

(1/20) benchmarking "2f_f_x_lg"...
done (took 1.430501154 seconds)
(2/20) benchmarking "2h_f_x_lg_noinline"...
done (took 1.095665714 seconds)
(3/20) benchmarking "6_two_field_access"...
done (took 1.111212046 seconds)
(4/20) benchmarking "1_baseline_simple"...
done (took 0.601532513 seconds)
(5/20) benchmarking "2b_pointwise_lg_j"...
done (took 1.084807648 seconds)
(6/20) benchmarking "10_vector_baseline"...
done (took 1.190719877 seconds)
(7/20) benchmarking "8_eight_field_access"...
done (took 0.61786503 seconds)
(8/20) benchmarking "2_full_lg_jacobian"...
done (took 0.616301259 seconds)
(9/20) benchmarking "7_four_field_access"...
done (took 0.64602214 seconds)
(10/20) benchmarking "2c_pointwise_lg_j_stack"...
done (took 1.123427484 seconds)
(11/20) benchmarking "3_full_lg_multiple"...
done (took 1.122109521 seconds)
(12/20) benchmarking "5_simplified_lg"...
done (took 1.042106413 seconds)
(13/20) benchmarking "2e_fd_localgeom_constructor"...
done (took 1.52913486 seconds)
(14/20) benchmarking "12_multiple_scalar_access"...
done (took 1.198834816 seconds)
(15/20) benchmarking "2d_pointwise_lg_j_noinline"...
done (took 1.109716723 seconds)
(16/20) benchmarking "2g_lambda_f_x_lg"...
done (took 1.165432238 seconds)
(17/20) benchmarking "11_project_full_lg"...
done (took 1.38707595 seconds)
(18/20) benchmarking "9_sixteen_field_access"...
done (took 0.623314003 seconds)
(19/20) benchmarking "4_extracted_j"...
done (took 0.606998772 seconds)
(20/20) benchmarking "2i_lambda_f_x_lg_noinline"...
done (took 1.097817211 seconds)

======================================================================
BENCHMARK RESULTS
======================================================================

SECTION 1: Basic Geometry Access
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  baseline_simple                     15.86 μs  (  +0.0% vs baseline)
  full_lg_jacobian                    16.85 μs  (  +6.2% vs baseline)
  2b_pointwise_lg_j                   17.39 μs  (  +9.6% vs baseline)
  2c_pointwise_lg_j_stack             17.29 μs  (  +9.0% vs baseline)
  2d_pointwise_lg_j_noinline          18.45 μs  ( +16.3% vs baseline)
  2e_fd_localgeom_constructor         13.79 μs  ( -13.1% vs baseline)
  2f_f_x_lg                           16.33 μs  (  +3.0% vs baseline)
  2g_lambda_f_x_lg                    16.85 μs  (  +6.2% vs baseline)
  2h_f_x_lg_noinline                  17.49 μs  ( +10.3% vs baseline)
  2i_lambda_f_x_lg_noinline           16.90 μs  (  +6.6% vs baseline)
  full_lg_multiple                    19.36 μs  ( +22.1% vs baseline)
  extracted_j                         16.90 μs  (  +6.6% vs baseline)
  simplified_lg                       17.12 μs  (  +7.9% vs baseline)

----------------------------------------------------------------------
SECTION 2: Struct Size Impact on Inlining
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  two_field_access                    16.98 μs  (  +7.1% vs baseline)
  four_field_access                   16.73 μs  (  +5.5% vs baseline)
  eight_field_access                  17.21 μs  (  +8.5% vs baseline)
  sixteen_field_access                17.17 μs  (  +8.3% vs baseline)

----------------------------------------------------------------------
SECTION 3: Projection Operations
----------------------------------------------------------------------

Execution Time (μs, lower is better):
  vector_baseline                     17.16 μs  (  +0.0% vs vec_baseline)
  project_full_lg                     17.24 μs  (  +0.5% vs vec_baseline)
  multiple_scalar_access              18.23 μs  (  +6.2% vs vec_baseline)

======================================================================
MEMORY FOOTPRINT COMPARISON
======================================================================

Data structure size per point:
  Scalar field:                    128.0 bytes
  TwoFieldGeom:                    256.0 bytes
  FourFieldGeom:                   512.0 bytes
  EightFieldGeom:                  1024.0 bytes
  SixteenFieldGeom:                2048.0 bytes
  Full LocalGeometry:              2688.0 bytes
  Extracted J:                     128.0 bytes

Total memory footprint:
  Scalar field:                    0.0001220703125 MB
  TwoFieldGeom:                    0.000244140625 MB (2.0x scalar)
  FourFieldGeom:                   0.00048828125 MB (4.0x scalar)
  EightFieldGeom:                  0.0009765625 MB (8.0x scalar)
  SixteenFieldGeom:                0.001953125 MB (16.0x scalar)
  Full LocalGeometry:              0.0025634765625 MB (21.0x scalar)
  Extracted J:                     0.0001220703125 MB (1.0x scalar)

======================================================================
ANALYSIS & KEY FINDINGS
======================================================================

1. BASIC GEOMETRY ACCESS OVERHEAD:
   Full LocalGeometry (J only):      6.2%
   Extracted J:                      6.6%

2. STRUCT SIZE IMPACT (accessing single field):
   TwoFieldGeom (16 bytes):          7.1%
   FourFieldGeom (32 bytes):         5.5%
   EightFieldGeom (64 bytes):        8.5%
   SixteenFieldGeom (128 bytes):     8.3%

   ✓ All struct sizes show similar performance - good inlining

3. PROJECTION OPERATIONS OVERHEAD:
   Covariant->Contravariant:         0.5%

======================================================================
RECOMMENDATIONS
======================================================================

⚠️  MODERATE OVERHEAD DETECTED:
   • LocalGeometry overhead: 6.2%
   • Projection overhead: 0.5%

   CONSIDER:
   1. Extract commonly-used components (J, WJ) at kernel entry
   2. Profile with nsys to see actual bandwidth/occupancy impact:
      ./scripts/run-nsys.sh --output=results/nsys/benchmark_lg \
          julia --project scripts/benchmark_local_geometry_impact.jl

======================================================================
CODE INSPECTION NOTES
======================================================================
To verify compiler inlining behavior:

1. Check LLVM IR for a simple kernel:
   julia> f(x, lg) = x + lg.J
   julia> @code_llvm f(1.0, first(local_geom_full))

   Look for: Should see direct field access, not function calls

2. Check PTX assembly for CUDA kernels:
   julia> using CUDA
   julia> kernel(x, lg) = (@inbounds x[1] += lg[1].J; nothing)
   julia> @device_code_ptx kernel(CuArray([1.0]), parent(local_geom_full))

   Look for: ld.param instructions (should be minimal for inlined structs)

3. Use nsys to profile actual memory bandwidth:
   ./scripts/run-nsys.sh --output=results/nsys/benchmark_lg \
       julia --project scripts/benchmark_local_geometry_impact.jl

   Then analyze with:
   nsys stats results/nsys/benchmark_lg.nsys-rep

   Look for: Memory bandwidth utilization, kernel occupancy

======================================================================

Markdown results written to: /home/pbachant/calkit/clima-local-geometry/scripts/../results/benchmark_local_geometry_impact.md

==========================================
Benchmark complete
End time: Tue Feb 24 06:35:34 PST 2026
==========================================
